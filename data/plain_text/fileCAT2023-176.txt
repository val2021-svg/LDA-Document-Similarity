      ST7 - 76 - HIGH PERFORMANCE SIMULATION    FOR FOOTPRINT REDUCTION       Dominante : MDS et INFONUM  Langue d’enseignement : English  Campus où le cours est proposé : Paris-Saclay       1.    General description of the thematic sequence   Simulation  is  nowadays  at  the  heart  of  many  design  and  optimization  approaches to reduce the footprint or the impact of the products created:  reduction of the risk of destruction in case of natural disasters, reduction of  an  airplane's  drag  to  minimize  the  fuel  consumed  and  the  CO2  emitted,  reduction of the time of a large-scale calculation and of the resources used  (the simulations themselves end up consuming a lot of energy on a lot of  processors),...   But these are often complex system simulations, which require both skills in  high performance and large scale simulations, and in optimization methods  to limit the scope of investigations and the hours of computation required.  Moreover, each study must still be done by seeking a compromise between  the quality of the solution found and the number of hours of computation  used,  because  hours  of  computation  are  expensive  (especially  on  highperformance  computation  clusters  or  on  a  large  scale  in  clouds).  It  is  therefore essential to learn how to manage a quota of computing hours.   This  ST  thus  includes  numerical  mathematics,  parallel  computing,  optimization  methods,  and  development  and  experimentation  on  PC  clusters or in clouds. Students will learn:   - to develop parallel modeling and simulations to reduce the duration of the  most expensive phase of the optimization loop,    -  to  associate  these  simulations  with  adapted  optimization  methods  and  algorithms,  which  will  minimize  the  number  of  configurations  to  be  simulated and evaluated (brute force approaches will be avoided)   - to experiment the programming of high performance computing platforms  (PC clusters or clouds),    -  to  exploit  these  platforms  under  the  constraint  of  a  quota  of  hours  of  calculations not to be exceeded.   535         2. Organization of the thematic sequence   2.1 Context and issues modules   The  modules  presenting  the  context  and  the  issues  will  begin  with  a  presentation of the objectives and organization of the ST. Then, a succession  of presentations by industrial partners will illustrate different cases of use of  high  performance  simulation  for  footprint  reduction  (reduction  of  energy  consumption,  reduction  of  financial  cost,  reduction  of  simulation  time,  reduction of data volumes...). On this occasion, the associated scientific and  technical  challenges  will  be  identified,  as  well  as  the  induced  needs  in  optimization.   The topics of the ST projects will also be presented during the context and  stakes modules: in aeronautics, in seismic exploration, in infrasound wave  detection,  in  risk  calculation  on  the  cloud,  and  in  temporal  and  energy  optimization of parallel calculations. These presentations will also present  the  economic  and  societal  issues  related  to  the  investments  required  to  carry out high-performance simulations, and the impact of simulation on the  evolution of the technologies that surround us.   A  visit  of  the  Very  Large  Computing  Center  (TGCC)  in  Bruyères-le-Châtel,  under the leadership of the CEA, will allow to see modern infrastructures of  high  performance  computing,  and  their  support  infrastructures  (power  supply,  cooling,  protections).  Finally,  a  round  table  with  all  the  industrial  partners will allow to discuss the trends for the future.   2.2 Specific course  : Parallel computing and optimization   This course includes mathematical, numerical, algorithmic and programming  aspects on parallel and distributed machines, associated with optimization  issues.   Among the notions covered, this course describes, in a first part, the basics  of parallel and distributed computing, by detailing in particular the computer  architectures and parallel programming models as well as the parallel and  distributed  algorithms  used  on  these  architectures.  In  a  second  part,  this  course  presents  parallel  optimization  methods  and  algorithms,  commonly  used  in  parallel  computing  codes.  Two  classes  of  methods  used  for  optimization  problems  are  successively  discussed,  namely  parallel  partitioning  and  domain  decomposition  methods,  and  genetic  algorithms  and parallel meta-heuristics. These methods and algorithms will be used in  the different integration courses of this ST, in order to deal with problems  from  the  engineering  sciences.  In  a  third  part,  this  course  focuses  on  the  performance  analysis  of  the  developed  solutions.  The  notions  of   536                  performance  metrics  and  scaling,  as  well  as  the  analysis  of  experimental  performances are also studied.   2.3 Industrial partners and proposed project topics   1.  The  CEA  DAM  (Direction  des  Applications  Militaires)  proposes  a  study  of  "Optimization  of  a  seismic  exploration  campaign  for  the  protection of structures”    Optimization of a seismic exploration   CEA-DAM is the French warning center for tsunamis and strong earthquakes,  and uses its high-performance computing resources for various missions.    After  the  Fukushima  accident  (Japan),  the  use  of  high  performance  computing  resources  has  become  more  and  more  common  for  the  estimation of the seismic risk associated with nuclear power plants: within  the framework of the design of new plants, but also in order to study the  performance of existing plants in the face of extreme events, not foreseen  at the time of their design.    This study concerns the optimization of a geophysical exploration campaign  on  an  experimental  site,  using  its  digital  twin.  The  project  consists  in  optimizing  (minimizing)  the  number  of  sensors  needed  to  discover  the  geological  configuration  of  the  site  of  interest.  Many  realistic  simulations  using  the  SEM3D  code  (Reverse  Time  Migration  method)  will  have  to  be  called  in  an  optimization  loop.  We  will therefore  look  for  an  optimization  method  that  allows  us  to  achieve  a  good  quality  of  optimization  while  respecting the time quota.   The calculations will be done on machines of the Moulon mesocenter, under  the constraint of a quota of calculation hours.   2.  ONERA  (Office  National  d'Etudes  et  Recherche  Aérospatial)  proposes  a  study  of  "Optimization  of  shapes  and  reduction  of  drag  in  aeronautics".   (Drag reduction in aeronautics)   Recent  studies  show  that  air  traffic  is  constantly  increasing.  Without  improvements in aircraft performance in terms of energy consumption, the  share of air transport in greenhouse gas emissions may become unbearable  in the future. Reducing aircraft fuel consumption requires both an increase   537      in  engine  efficiency  and  an  improvement  in  aircraft  aerodynamics  and  weight reduction.    Numerical tools have been widely used for a long time in the aeronautical  field  to  help  in  the  design  and  optimization  of  systems.  For  example,  the  shape of a wing can be improved in order to reduce its drag, at constant lift,  or its internal structure can be lightened. The optimization methods require  successive  calculations  for  different  wing  geometries.  The  higher  the  accuracy of the numerical models, the higher the computational costs for  each  step.  The  only  way  to  reduce  the  computation  time  to  be  able  to  integrate the optimization methods in the industrial design cycle is to use  parallel  computers.  The  objective  of  this  project  is  to  realize  the  parallelization of the most expensive part of the optimization phase, i.e. the  resolution  of  large  linear  systems  from  finite  element  models  on  large  meshes, and to experiment with different sets of optimization parameters.   The tests will be carried out on the parallel machines of the CentraleSupélec  Teaching Data Center, under the constraint of a quota of computing hours.   3.  The  CEA  DAM  (Direction  des  Applications  Militaires)  proposes  a  study of "Optimization of infrasound detection for the verification of the  Comprehensive Nuclear Test Ban Treaty".   Detection of infrasound waves   CEA-DAM is the French warning center for tsunamis and strong earthquakes,  and also participates in the implementation of the verification means of the  Comprehensive  Nuclear  Test  Ban  Treaty  (CTBT)  by  using  its  high  performance computing resources.    A  parallel  compressible  hydrodynamics  code  has  been  developed  at  CEA  DAM,  which  allows  the  simulation  of  blast  wave  and  acoustic  wave  propagation in the presence of terrain and buildings, with or without wind.  On the other hand, it is considered that judiciously placed sensors allow the  recording  of  overpressure  signals  in  case  of  an  explosion.  Two  types  of  problems  can  then  be  studied:  (1)  find  the  location  of  an  explosion  and  determine its power from the recordings of sensors located in the field, (2)  define  where  to  judiciously  position  sensors  to  maximize  the  chances  of  detecting an explosion within a given area.    A  "brute  force"  investigation  simulating  all  possible  configurations  of  the  parameters  would  consume  too  many  hours  of  computation.  We  will  therefore  develop  an  optimization  loop  exploring  the  space  of  possible  configurations sparingly and using the hydrodynamics code efficiently.   The computations will be done on CEA clusters under constraints of a quota  of computation hours, and three of the last days of the study will take place  on the Bruyères-le-Châtel site. This study is reserved for students from the  European Union.   538         4.  ANEO is an IT company expert in high performance computing and  cloud  operation,  which  proposes  a  study  of  "Energy  optimization  and  acceleration of a financial computation graph on cloud".   Graph of financial calculations   One of the difficulties in assessing the accounts of an insurance company (or  a bank) lies in the valuation of financial assets (shares, life or car insurance  contracts, etc.) and the underlying risks. Depending on the valuation of the  risks taken, the regulations resulting from the various economic crises oblige  the insurance company or the bank to immobilize a certain amount of equity  capital.    The steps of such a calculation, managed by ANEO, form a task graph with  numerous dependencies, and the sum of the execution times corresponds  to  the  equivalent  of  413177  hours  of  calculation.  On  an  infrastructure  of  1700 cores the computation time would be a little more than 10 full days if  all cores could work at any time. But because of the dependencies between  the  computational  tasks,  it  happens  that  there  are  not  enough  tasks  to  occupy all the allocated resources, and the process finally takes more than  10 days.   In order to optimize the cost of this computation, we want to: (1) use ondemand resources available in the cloud, and (2) optimize the execution of  the task graph by looking for the best turn-on/off strategy for the compute  nodes, and the best scheduling of tasks on the available nodes. To do this,  we will develop a cost function that calculates the execution time of the task  graph as a function of a node management and task scheduling strategy, and  we  will  implement  an  optimization  algorithm  that  seeks  the  best  parameterization of this strategy.   INTEL proposes a study of "Low Cost Optimization of Acoustic Wave   5.  Propagation Code Performance".   Reducing the footprint of a code   Any high performance application running on a parallel machine has many  configuration parameters in its source code and compilation, which have a  significant impact on its performance and energy footprint. But the behavior  of the application depends on the architecture of the processors used, the  test case data and the software configuration of the machine. In the end,  this  behavior  is  extremely  difficult  to  model,  and  the  configuration  parameter  space  can  be  very  large.  The  use  of  optimization  algorithms  therefore appears fundamental to converge towards a configuration of the  application  minimizing  its  execution  time  and  its  energy  footprint  on  the  machine used.    539      However, each execution of a test case of an HPC application can be long,  even on a parallel machine. We will therefore target optimization methods  that are not too greedy in terms of the number of experiments, so that the  pre-stage  of  optimizing  the  HPC  code  does  not  itself  consume  too  many  computing resources! This amounts to looking for a compromise between  the  energy  spent  to  optimize  an  HPC  code,  and  the  energy  saved  by  this  application once optimized.   The tests will be carried out on the parallel machines of CentraleSupélec's  Teaching Data Center, under the constraint of a quota of computing hours.   540   